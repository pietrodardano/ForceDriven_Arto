Building Function:
def build_branched_model(input_shapes):
    def create_branch(input_shape, branch_id):
        input_layer = Input(shape=input_shape, name=f'input{branch_id}')
        x = Conv1D(filters=128, kernel_size=400, strides=100, activation='relu', padding='same', name=f'conv1d_{branch_id}_1')(input_layer)
        #x = MaxPooling1D(pool_size=2)(x)
        x = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu', name=f'conv1d_{branch_id}_2')(x)
        #x = Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', name=f'conv1d_{branch_id}_3')(x)
        #x = MaxPooling1D(pool_size=4)(x)
        x = Dropout(rate=0.3, name=f'dropout_{branch_id}_1')(x)
        x = Conv1D(filters=256, kernel_size=2, strides=1, activation='relu', name=f'conv1d_{branch_id}_4')(x)
        x = GlobalMaxPooling1D(name=f'gap1d_{branch_id}_1')(x)
        return input_layer, x

    inputs = []
    branches = []
    
    for i, input_shape in enumerate(input_shapes, 1):
        input_layer, branch_output = create_branch(input_shape, i)
        inputs.append(input_layer)
        branches.append(branch_output)
    
    merged = concatenate(branches, name='concatenate_1')
    
    # Dense layers
    dense = Dense(128, activation='relu', name='dense_1')(merged)
    #dense = Dense(32, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=inputs, outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = np.dstack((data[:,:,2], data[:,:,4]))
        globals()[f"{key}2"] = np.dstack((data[:,:,0], data[:,:,2]))
        globals()[f"{key}3"] = data[:,:,5]
        globals()[f"{key}4"] = np.dstack((data[:,:,1], data[:,:,3]))


Model: "functional_5"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 20, 128)   │    102,528 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 20, 128)   │    102,528 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 20, 128)   │     51,328 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 20, 128)   │    102,528 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 3, 128)    │    262,272 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 3, 128)    │    262,272 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 3, 128)    │    262,272 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 3, 128)    │    262,272 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 3, 128)    │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 3, 128)    │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 3, 128)    │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_4_1         │ (None, 3, 128)    │          0 │ conv1d_4_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_4 (Conv1D) │ (None, 2, 256)    │     65,792 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_4 (Conv1D) │ (None, 2, 256)    │     65,792 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_4 (Conv1D) │ (None, 2, 256)    │     65,792 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_4 (Conv1D) │ (None, 2, 256)    │     65,792 │ dropout_4_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_4[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0],  │
│                     │                   │            │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │    131,200 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        774 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 5,409,428 (20.64 MB)
 Trainable params: 1,803,142 (6.88 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,606,286 (13.76 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f0e385481f0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.10107694566249847
Test val_loss: 0.46939554810523987
Train accuracy: 0.9572192430496216
Accuracy Score: 0.9625668449197861
F1 Score: 0.9568810593751736
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.95      0.97        40
         1.0       0.98      0.96      0.97        54
         2.0       0.95      0.98      0.96        42
         3.0       0.93      1.00      0.97        14
         4.0       1.00      0.87      0.93        15
         5.0       0.88      1.00      0.94        22

    accuracy                           0.96       187
   macro avg       0.96      0.96      0.96       187
weighted avg       0.96      0.96      0.96       187

Training History:
accuracy: [0.48128342628479004, 0.698752224445343, 0.8003565073013306, 0.8199643492698669, 0.8235294222831726, 0.8413547277450562, 0.8841354846954346, 0.8912655711174011, 0.8770053386688232, 0.9055258631706238, 0.8859180212020874, 0.8698752522468567, 0.8877005577087402, 0.8787878751754761, 0.916221022605896, 0.9073083996772766, 0.9019607901573181, 0.8948306441307068, 0.9304812550544739, 0.939393937587738, 0.939393937587738, 0.9286987781524658, 0.9340463280677795, 0.9376114010810852, 0.9376114010810852, 0.9447415471076965, 0.9322637915611267, 0.9483066201210022, 0.9607843160629272, 0.9590017795562744, 0.9643493890762329, 0.9465240836143494, 0.9554367065429688, 0.9536541700363159, 0.9714794754981995, 0.9554367065429688, 0.9518716335296631, 0.9554367065429688, 0.9590017795562744, 0.9732620120048523, 0.9572192430496216, 0.9304812550544739, 0.9536541700363159, 0.9554367065429688, 0.9518716335296631, 0.9518716335296631, 0.9447415471076965, 0.9625668525695801, 0.9536541700363159, 0.9572192430496216]
loss: [1.3187577724456787, 0.8164069652557373, 0.5407391786575317, 0.45968127250671387, 0.5184682011604309, 0.4033147692680359, 0.32042160630226135, 0.33344316482543945, 0.30323678255081177, 0.25088995695114136, 0.28532513976097107, 0.3643457591533661, 0.2810541093349457, 0.3235713243484497, 0.23685592412948608, 0.2605840563774109, 0.26182541251182556, 0.27989163994789124, 0.1468227505683899, 0.16048875451087952, 0.14488036930561066, 0.19395624101161957, 0.20135559141635895, 0.15110236406326294, 0.1580224633216858, 0.1834246963262558, 0.2042803168296814, 0.1563742458820343, 0.10567077249288559, 0.1010056734085083, 0.10477954894304276, 0.1421162635087967, 0.10002779960632324, 0.09828054159879684, 0.07076726853847504, 0.11799609661102295, 0.14083731174468994, 0.1034247875213623, 0.10291345417499542, 0.07344088703393936, 0.165843665599823, 0.22453536093235016, 0.1289833039045334, 0.11174706369638443, 0.10438396036624908, 0.14133574068546295, 0.136333167552948, 0.10244274884462357, 0.14944863319396973, 0.10107694566249847]
val_accuracy: [0.625668466091156, 0.6898396015167236, 0.8235294222831726, 0.7967914342880249, 0.7967914342880249, 0.8181818127632141, 0.8288770318031311, 0.8823529481887817, 0.8716577291488647, 0.8877005577087402, 0.8395721912384033, 0.866310179233551, 0.8502673506736755, 0.8342245817184448, 0.8502673506736755, 0.903743326663971, 0.8502673506736755, 0.866310179233551, 0.893048107624054, 0.8823529481887817, 0.9144384860992432, 0.8877005577087402, 0.8823529481887817, 0.8823529481887817, 0.8716577291488647, 0.893048107624054, 0.8983957171440125, 0.893048107624054, 0.9090909361839294, 0.9197860956192017, 0.893048107624054, 0.9090909361839294, 0.9197860956192017, 0.8877005577087402, 0.9197860956192017, 0.9251337051391602, 0.8983957171440125, 0.9251337051391602, 0.9144384860992432, 0.903743326663971, 0.8877005577087402, 0.8770053386688232, 0.893048107624054, 0.9090909361839294, 0.893048107624054, 0.8502673506736755, 0.8770053386688232, 0.9090909361839294, 0.9251337051391602, 0.9197860956192017]
val_loss: [0.8504948019981384, 0.6413976550102234, 0.5443122386932373, 0.5545554757118225, 0.5741888880729675, 0.4475221633911133, 0.4749037027359009, 0.3439514636993408, 0.40067258477211, 0.35293248295783997, 0.5421503782272339, 0.3621293306350708, 0.4417514204978943, 0.41285693645477295, 0.39851143956184387, 0.2933495342731476, 0.4254091680049896, 0.3627481162548065, 0.36646878719329834, 0.3119660019874573, 0.3307898938655853, 0.3282371461391449, 0.38848787546157837, 0.3491789400577545, 0.43728891015052795, 0.3299303650856018, 0.2830829918384552, 0.39005059003829956, 0.3620052635669708, 0.3864677846431732, 0.5251282453536987, 0.3623271584510803, 0.319411963224411, 0.5182445645332336, 0.44722434878349304, 0.30794668197631836, 0.4752352237701416, 0.2666376829147339, 0.33701974153518677, 0.41239026188850403, 0.42786991596221924, 0.5318580269813538, 0.4586913585662842, 0.3719220757484436, 0.443311482667923, 0.6133835911750793, 0.5454221367835999, 0.49938398599624634, 0.351238876581192, 0.46939554810523987]

################################################################################################ 

