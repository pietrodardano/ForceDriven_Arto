Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(128, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional_15"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_44          │ (None, 256)       │          0 │ gap1d_1_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_45          │ (None, 256)       │          0 │ gap1d_2_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_46          │ (None, 256)       │          0 │ gap1d_3_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_47          │ (None, 256)       │          0 │ gap1d_4_1[0][0]   │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ flatten_44[0][0], │
│ (Concatenate)       │                   │            │ flatten_45[0][0], │
│                     │                   │            │ flatten_46[0][0], │
│                     │                   │            │ flatten_47[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 128)       │    131,200 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │        516 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 2,064,782 (7.88 MB)
 Trainable params: 688,260 (2.63 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,376,522 (5.25 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f21a4448970>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.1087242141366005
Test val_loss: 0.3681829571723938
Train accuracy: 0.9573643207550049
Accuracy Score: 0.9883720930232558
F1 Score: 0.9871050934880722
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        17
         1.0       1.00      1.00      1.00        25
         2.0       0.96      1.00      0.98        23
         3.0       1.00      1.00      1.00        21

    accuracy                           0.99        86
   macro avg       0.99      0.99      0.99        86
weighted avg       0.99      0.99      0.99        86

Training History:
accuracy: [0.44186046719551086, 0.6666666865348816, 0.748062014579773, 0.6976743936538696, 0.6472868323326111, 0.7093023061752319, 0.6782945990562439, 0.7558139562606812, 0.8410852551460266, 0.7945736646652222, 0.7286821603775024, 0.7751938104629517, 0.7984496355056763, 0.8372092843055725, 0.8759689927101135, 0.8798449635505676, 0.930232584476471, 0.9457364082336426, 0.9224806427955627, 0.8875969052314758, 0.8914728760719299, 0.8992248177528381, 0.8837209343910217, 0.9263566136360168, 0.9457364082336426, 0.9457364082336426, 0.9496123790740967, 0.9496123790740967, 0.8798449635505676, 0.930232584476471, 0.9651162624359131, 0.9534883499145508, 0.934108555316925, 0.9767441749572754, 0.9651162624359131, 0.9767441749572754, 0.9573643207550049]
loss: [1.3593906164169312, 0.9415977001190186, 0.6514079570770264, 0.7669812440872192, 0.9029785990715027, 0.7203240394592285, 0.680518388748169, 0.5152295231819153, 0.43413764238357544, 0.5316542387008667, 0.551618218421936, 0.49101853370666504, 0.4412585496902466, 0.33340609073638916, 0.34103092551231384, 0.30040237307548523, 0.23557811975479126, 0.20130512118339539, 0.21947403252124786, 0.27477917075157166, 0.30538612604141235, 0.27254217863082886, 0.2646591067314148, 0.184284508228302, 0.14689692854881287, 0.15862813591957092, 0.15255647897720337, 0.1554492712020874, 0.2680147886276245, 0.2033766210079193, 0.11442045867443085, 0.1257983148097992, 0.12671205401420593, 0.12107909470796585, 0.10657943040132523, 0.09407728910446167, 0.1087242141366005]
val_accuracy: [0.5232558250427246, 0.6860465407371521, 0.604651153087616, 0.7674418687820435, 0.604651153087616, 0.7441860437393188, 0.6627907156944275, 0.6860465407371521, 0.6627907156944275, 0.6511628031730652, 0.7209302186965942, 0.7790697813034058, 0.8139534592628479, 0.7790697813034058, 0.8139534592628479, 0.8372092843055725, 0.8837209343910217, 0.8604651093482971, 0.8720930218696594, 0.7558139562606812, 0.7906976938247681, 0.8837209343910217, 0.9186046719551086, 0.8837209343910217, 0.895348846912384, 0.8720930218696594, 0.8372092843055725, 0.8255813717842102, 0.9186046719551086, 0.9186046719551086, 0.895348846912384, 0.8837209343910217, 0.895348846912384, 0.895348846912384, 0.895348846912384, 0.8604651093482971, 0.8488371968269348]
val_loss: [1.1205575466156006, 0.8111328482627869, 0.8425612449645996, 0.6146861910820007, 1.227007269859314, 0.6873382329940796, 0.6495935916900635, 0.5399453043937683, 0.6710013747215271, 0.6289337277412415, 0.47890183329582214, 0.4570121467113495, 0.449413925409317, 0.44140803813934326, 0.44375067949295044, 0.36310938000679016, 0.31558161973953247, 0.33374133706092834, 0.2574193477630615, 0.5644548535346985, 0.557162880897522, 0.2998864948749542, 0.23043105006217957, 0.24642327427864075, 0.2216261476278305, 0.2929002046585083, 0.36742082238197327, 0.502358078956604, 0.20970667898654938, 0.23216992616653442, 0.25765326619148254, 0.2851267158985138, 0.28448963165283203, 0.29762086272239685, 0.22527432441711426, 0.3552158772945404, 0.3681829571723938]

################################################################################################ 

