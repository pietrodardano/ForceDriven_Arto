Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3, input_shape4):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    x1 = Flatten()(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    x2 = Flatten()(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    x3 = Flatten()(x3)
    
    # Fourth input branch
    input4 = Input(shape=input_shape4, name='input4')
    x4 = Conv1D(filters=64*FILTN, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_4_1')(input4)
    x4 = Conv1D(filters=128*FILTN, kernel_size=4, strides=2, activation='relu', name='conv1d_4_2')(x4)
    x4 = GlobalMaxPooling1D(name='gap1d_4_1')(x4)
    x4 = Flatten()(x4)
    
    # Concatenate the outputs of the four branches
    merged = concatenate([x1, x2, x3, x4], name='concatenate_1')
    
    # Dense layers
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(OUT_N, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3, input4], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 0]
        globals()[f"{key}2"] = data[:, :, 1]
        globals()[f"{key}3"] = np.dstack((data[:, :, 2], data[:, :, 4]))
        globals()[f"{key}4"] = np.dstack((data[:, :, 6], data[:, :, 8]))


Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 3000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input4 (InputLayer) │ (None, 3000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 300, 128)  │      5,248 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_1 (Conv1D) │ (None, 300, 128)  │     10,368 │ input4[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_4_2 (Conv1D) │ (None, 149, 256)  │    131,328 │ conv1d_4_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 256)       │          0 │ conv1d_1_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 256)       │          0 │ conv1d_2_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 256)       │          0 │ conv1d_3_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_4_1           │ (None, 256)       │          0 │ conv1d_4_2[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten (Flatten)   │ (None, 256)       │          0 │ gap1d_1_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_1 (Flatten) │ (None, 256)       │          0 │ gap1d_2_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_2 (Flatten) │ (None, 256)       │          0 │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_3 (Flatten) │ (None, 256)       │          0 │ gap1d_4_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1024)      │          0 │ flatten[0][0],    │
│ (Concatenate)       │                   │            │ flatten_1[0][0],  │
│                     │                   │            │ flatten_2[0][0],  │
│                     │                   │            │ flatten_3[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     65,600 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 4)         │        260 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 1,867,214 (7.12 MB)
 Trainable params: 622,404 (2.37 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,244,810 (4.75 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7fb61c1071c0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.09400416165590286
Test val_loss: 0.4636473059654236
Train accuracy: 0.9689922332763672
Accuracy Score: 0.9883720930232558
F1 Score: 0.9871050934880722
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        17
         1.0       1.00      1.00      1.00        25
         2.0       0.96      1.00      0.98        23
         3.0       1.00      1.00      1.00        21

    accuracy                           0.99        86
   macro avg       0.99      0.99      0.99        86
weighted avg       0.99      0.99      0.99        86

Training History:
accuracy: [0.39147287607192993, 0.569767415523529, 0.5310077667236328, 0.643410861492157, 0.7325581312179565, 0.7558139562606812, 0.7403100728988647, 0.7093023061752319, 0.7015503644943237, 0.7325581312179565, 0.8062015771865845, 0.8643410801887512, 0.8062015771865845, 0.8527131676673889, 0.8100775480270386, 0.8604651093482971, 0.856589138507843, 0.8527131676673889, 0.8914728760719299, 0.8023256063461304, 0.748062014579773, 0.8527131676673889, 0.8449612259864807, 0.8255813717842102, 0.8798449635505676, 0.9186046719551086, 0.895348846912384, 0.9224806427955627, 0.9108527302742004, 0.9379844665527344, 0.9457364082336426, 0.9108527302742004, 0.8914728760719299, 0.9031007885932922, 0.9457364082336426, 0.9457364082336426, 0.9651162624359131, 0.9534883499145508, 0.8992248177528381, 0.9457364082336426, 0.9573643207550049, 0.9767441749572754, 0.961240291595459, 0.9573643207550049, 0.9418604373931885, 0.9573643207550049, 0.9689922332763672, 0.9651162624359131, 0.9767441749572754, 0.9728682041168213, 0.9689922332763672]
loss: [1.341345191001892, 0.9495832920074463, 1.1030054092407227, 0.7852323651313782, 0.6616461277008057, 0.5839545130729675, 0.5568005442619324, 0.6145615577697754, 0.6413784623146057, 0.556799590587616, 0.47123152017593384, 0.3911881446838379, 0.42144474387168884, 0.37830159068107605, 0.428369402885437, 0.35855451226234436, 0.3610895872116089, 0.32955360412597656, 0.2796560525894165, 0.4033854901790619, 0.5730128884315491, 0.387554794549942, 0.34747326374053955, 0.3414856493473053, 0.28605520725250244, 0.24794580042362213, 0.25585946440696716, 0.2052435427904129, 0.21041463315486908, 0.17321813106536865, 0.1623513400554657, 0.2354995161294937, 0.26511383056640625, 0.21226316690444946, 0.16116970777511597, 0.1283712387084961, 0.12651370465755463, 0.12842869758605957, 0.2193244993686676, 0.1395338773727417, 0.11169195920228958, 0.10800036787986755, 0.11587611585855484, 0.11608369648456573, 0.13444805145263672, 0.09897330403327942, 0.10356184095144272, 0.09727590531110764, 0.08717309683561325, 0.08956253528594971, 0.09400416165590286]
val_accuracy: [0.4651162922382355, 0.44186046719551086, 0.6627907156944275, 0.6279069781303406, 0.6511628031730652, 0.6279069781303406, 0.6511628031730652, 0.5232558250427246, 0.7558139562606812, 0.7674418687820435, 0.8488371968269348, 0.6395348906517029, 0.8139534592628479, 0.8139534592628479, 0.8255813717842102, 0.7906976938247681, 0.7441860437393188, 0.7906976938247681, 0.8023256063461304, 0.8488371968269348, 0.8023256063461304, 0.7790697813034058, 0.8604651093482971, 0.6744186282157898, 0.8720930218696594, 0.8604651093482971, 0.8372092843055725, 0.8720930218696594, 0.8837209343910217, 0.895348846912384, 0.8023256063461304, 0.7790697813034058, 0.7906976938247681, 0.8837209343910217, 0.8837209343910217, 0.9069767594337463, 0.9069767594337463, 0.8604651093482971, 0.8488371968269348, 0.9069767594337463, 0.9069767594337463, 0.895348846912384, 0.930232584476471, 0.8372092843055725, 0.9069767594337463, 0.8604651093482971, 0.9186046719551086, 0.895348846912384, 0.9186046719551086, 0.8720930218696594, 0.8255813717842102]
val_loss: [1.1173171997070312, 1.2139537334442139, 0.819786012172699, 0.7542999386787415, 0.6238412857055664, 0.6321616768836975, 0.6575460433959961, 0.8555010557174683, 0.5728387832641602, 0.5231804847717285, 0.42536571621894836, 0.6000649333000183, 0.49846702814102173, 0.4459187686443329, 0.3530028760433197, 0.41099312901496887, 0.4382598400115967, 0.46493956446647644, 0.39304035902023315, 0.33170491456985474, 0.5412898659706116, 0.3887022137641907, 0.3448064923286438, 0.4814760088920593, 0.3211563527584076, 0.298115998506546, 0.3079235851764679, 0.2755542993545532, 0.30005189776420593, 0.28986892104148865, 0.41804584860801697, 0.5111103653907776, 0.541715145111084, 0.2787599265575409, 0.26723742485046387, 0.2695316672325134, 0.2576748728752136, 0.30322185158729553, 0.3734740912914276, 0.26769450306892395, 0.23457181453704834, 0.28854072093963623, 0.22274154424667358, 0.4585503339767456, 0.23754306137561798, 0.27534133195877075, 0.2513001561164856, 0.3090415894985199, 0.2519674003124237, 0.26933935284614563, 0.4636473059654236]

Confusion Matrix:
[[16  0  1  0]
 [ 0 25  0  0]
 [ 0  0 23  0]
 [ 0  0  0 21]]

################################################################################################ 

