Building Function:
def build_branched_model(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*2, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = Conv1D(filters=128*2, kernel_size=8, strides=2, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.2, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=256*2, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*2, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = Conv1D(filters=128*2, kernel_size=8, strides=2, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.2, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=256*2, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*2, kernel_size=40, strides=10, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = Conv1D(filters=128*2, kernel_size=8, strides=2, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.2, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=256*2, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    
    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')
    
    # Dense layer
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(6, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3], outputs=output)
    return model


Assign and Deploy Variables Function:
def assign_and_deploy_variables(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 5]
        globals()[f"{key}2"] = np.dstack((data[:, :, 0], data[:, :, 4], data[:, :, 2]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 1], data[:, :, 3]))


Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 3)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 200, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 200, 128)  │     15,488 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 200, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 97, 256)   │    262,400 │ conv1d_1_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 97, 256)   │    262,400 │ conv1d_2_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 97, 256)   │    262,400 │ conv1d_3_1[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 97, 256)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 97, 256)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 97, 256)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 96, 512)   │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 96, 512)   │    262,656 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 96, 512)   │    262,656 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 512)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 512)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1536)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     98,368 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_2 (Dense)     │ (None, 16)        │      1,040 │ dense_1[0][0]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        102 │ dense_2[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 5,117,348 (19.52 MB)
 Trainable params: 1,705,782 (6.51 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,411,566 (13.01 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f0d4cffc8b0>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.35912632942199707
Test val_loss: 0.9448215365409851
Train accuracy: 0.8541114330291748
Accuracy Score: 0.7936507936507936
F1 Score: 0.780924394754182
Classification Report:
               precision    recall  f1-score   support

         0.0       0.87      0.95      0.91        21
         1.0       0.87      0.87      0.87        39
         2.0       0.62      0.76      0.68        21
         3.0       0.90      0.82      0.86        11
         4.0       0.93      0.81      0.87        16
         5.0       0.57      0.44      0.50        18

    accuracy                           0.79       126
   macro avg       0.79      0.78      0.78       126
weighted avg       0.80      0.79      0.79       126

Training History:
accuracy: [0.36339521408081055, 0.38461539149284363, 0.48806366324424744, 0.47480106353759766, 0.5331565141677856, 0.575596809387207, 0.5915119647979736, 0.6419098377227783, 0.6153846383094788, 0.6180371642112732, 0.6763925552368164, 0.6604774594306946, 0.6870026588439941, 0.6949602365493774, 0.6896551847457886, 0.6790450811386108, 0.7108753323554993, 0.7161803841590881, 0.7161803841590881, 0.7718833088874817, 0.7824933528900146, 0.7957559823989868, 0.8302386999130249, 0.7877984046936035, 0.8090185523033142, 0.8541114330291748, 0.8488063812255859, 0.8567638993263245, 0.8779841065406799, 0.912466824054718, 0.883289098739624, 0.9177718758583069, 0.9151193499565125, 0.9177718758583069, 0.883289098739624, 0.9257294535636902, 0.9018567800521851, 0.9098142981529236, 0.941644549369812, 0.9496021270751953, 0.9230769276618958, 0.9575597047805786, 0.8992042541503906, 0.8408488035202026, 0.7798408269882202, 0.8037135004997253, 0.8196286559104919, 0.8541114330291748]
loss: [1.5399771928787231, 1.367882251739502, 1.2543058395385742, 1.2238556146621704, 1.149987816810608, 1.062074065208435, 0.9859325885772705, 0.9375724196434021, 0.9161413908004761, 0.8793484568595886, 0.7904132008552551, 0.8162702322006226, 0.755253255367279, 0.7212512493133545, 0.7404621839523315, 0.8071243166923523, 0.6898508071899414, 0.6293851137161255, 0.6867744326591492, 0.5540759563446045, 0.5237525105476379, 0.501471221446991, 0.40816304087638855, 0.5182892084121704, 0.49664825201034546, 0.3794950544834137, 0.3699454665184021, 0.3951472342014313, 0.3225243091583252, 0.2514939308166504, 0.3193502426147461, 0.2365410178899765, 0.22585409879684448, 0.2317284494638443, 0.3320918679237366, 0.19088014960289001, 0.22625228762626648, 0.22317703068256378, 0.17421863973140717, 0.14363089203834534, 0.17940469086170197, 0.11787259578704834, 0.2800070643424988, 0.5354591012001038, 0.7712936401367188, 0.54962158203125, 0.5261357426643372, 0.35912632942199707]
val_accuracy: [0.380952388048172, 0.4047619104385376, 0.420634925365448, 0.4047619104385376, 0.5555555820465088, 0.4126984179019928, 0.5396825671195984, 0.5, 0.5158730149269104, 0.5873016119003296, 0.5952380895614624, 0.5079365372657776, 0.5952380895614624, 0.6269841194152832, 0.4444444477558136, 0.6269841194152832, 0.6111111044883728, 0.6111111044883728, 0.6190476417541504, 0.7460317611694336, 0.6507936716079712, 0.6507936716079712, 0.7460317611694336, 0.7460317611694336, 0.6984127163887024, 0.7222222089767456, 0.7777777910232544, 0.6507936716079712, 0.6746031641960144, 0.7857142686843872, 0.7063491940498352, 0.8333333134651184, 0.6746031641960144, 0.7777777910232544, 0.8015872836112976, 0.8095238208770752, 0.7698412537574768, 0.7777777910232544, 0.761904776096344, 0.761904776096344, 0.7857142686843872, 0.7936508059501648, 0.817460298538208, 0.7301587462425232, 0.7301587462425232, 0.682539701461792, 0.5873016119003296, 0.7857142686843872]
val_loss: [1.5190023183822632, 1.3731474876403809, 1.3237524032592773, 1.2632262706756592, 1.2055845260620117, 1.22812819480896, 1.1202913522720337, 1.1950265169143677, 1.2226723432540894, 1.0532952547073364, 1.0668175220489502, 1.1251007318496704, 1.0552978515625, 1.0236945152282715, 1.2970049381256104, 0.9048323035240173, 1.0809406042099, 1.1392054557800293, 0.9759703278541565, 0.8323037028312683, 0.9923437237739563, 0.9029667973518372, 0.9162694215774536, 1.0330796241760254, 0.8730952739715576, 0.8443582653999329, 0.8257132172584534, 1.0250842571258545, 0.9473266005516052, 0.9237551093101501, 0.8800505995750427, 0.8174670934677124, 1.0937749147415161, 0.7793561220169067, 0.8877139687538147, 0.7569689154624939, 0.9762168526649475, 0.8697258830070496, 1.0048227310180664, 1.1226475238800049, 1.0053519010543823, 1.1440471410751343, 1.2378225326538086, 1.173814058303833, 1.121857762336731, 1.6065138578414917, 1.2822664976119995, 0.9448215365409851]

Confusion Matrix:
[[20  1  0  0  0  0]
 [ 3 34  0  0  0  2]
 [ 0  1 16  0  0  4]
 [ 0  0  1  9  1  0]
 [ 0  0  2  1 13  0]
 [ 0  3  7  0  0  8]]

################################################################################################ 

