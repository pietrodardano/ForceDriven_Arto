def assign_and_deploy_variables_v2(data_dict):
    for key, data in data_dict.items():
        globals()[f"{key}1"] = data[:, :, 5]
        globals()[f"{key}2"] = np.dstack((data[:, :, 0], data[:, :, 4], data[:, :, 2]))
        globals()[f"{key}3"] = np.dstack((data[:, :, 1], data[:, :, 3]))

def build_branched_model(input_shape1, input_shape2, input_shape3):
    # First input branch
    input1 = Input(shape=input_shape1, name='input1')
    x1 = Conv1D(filters=64*2, kernel_size=40, strides=20, activation='relu', padding='same', name='conv1d_1_1')(input1)
    x1 = MaxPooling1D(pool_size=2, name='maxpool1d_1_1')(x1)
    x1 = Conv1D(filters=128*2, kernel_size=8, strides=4, activation='relu', name='conv1d_1_2')(x1)
    x1 = Dropout(rate=0.2, name='dropout_1_1')(x1)
    x1 = Conv1D(filters=256*2, kernel_size=2, strides=1, activation='relu', name='conv1d_1_3')(x1)
    x1 = GlobalMaxPooling1D(name='gap1d_1_1')(x1)
    
    # Second input branch
    input2 = Input(shape=input_shape2, name='input2')
    x2 = Conv1D(filters=64*2, kernel_size=40, strides=20, activation='relu', padding='same', name='conv1d_2_1')(input2)
    x2 = MaxPooling1D(pool_size=2, name='maxpool1d_2_1')(x2)
    x2 = Conv1D(filters=128*2, kernel_size=8, strides=4, activation='relu', name='conv1d_2_2')(x2)
    x2 = Dropout(rate=0.2, name='dropout_2_1')(x2)
    x2 = Conv1D(filters=256*2, kernel_size=2, strides=1, activation='relu', name='conv1d_2_3')(x2)
    x2 = GlobalMaxPooling1D(name='gap1d_2_1')(x2)
    
    # Third input branch
    input3 = Input(shape=input_shape3, name='input3')
    x3 = Conv1D(filters=64*2, kernel_size=40, strides=20, activation='relu', padding='same', name='conv1d_3_1')(input3)
    x3 = MaxPooling1D(pool_size=2, name='maxpool1d_3_1')(x3)
    x3 = Conv1D(filters=128*2, kernel_size=8, strides=4, activation='relu', name='conv1d_3_2')(x3)
    x3 = Dropout(rate=0.2, name='dropout_3_1')(x3)
    x3 = Conv1D(filters=256*2, kernel_size=2, strides=1, activation='relu', name='conv1d_3_3')(x3)
    x3 = GlobalMaxPooling1D(name='gap1d_3_1')(x3)
    
    # Concatenate the outputs of the three branches
    merged = concatenate([x1, x2, x3], name='concatenate_1')
    
    # Dense layer
    dense = Dense(64, activation='relu', name='dense_1')(merged)
    #dense = Dense(16, activation='relu', name='dense_2')(dense)
    
    # Output layer for 6-class classification
    output = Dense(6, activation='softmax', name='output')(dense)
    
    model = Model(inputs=[input1, input2, input3], outputs=output)
    return model

Model: "functional_4"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input1 (InputLayer) │ (None, 2000, 1)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input2 (InputLayer) │ (None, 2000, 3)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input3 (InputLayer) │ (None, 2000, 2)   │          0 │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_1 (Conv1D) │ (None, 100, 128)  │      5,248 │ input1[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_1 (Conv1D) │ (None, 100, 128)  │     15,488 │ input2[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_1 (Conv1D) │ (None, 100, 128)  │     10,368 │ input3[0][0]      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_1_1       │ (None, 50, 128)   │          0 │ conv1d_1_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_2_1       │ (None, 50, 128)   │          0 │ conv1d_2_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ maxpool1d_3_1       │ (None, 50, 128)   │          0 │ conv1d_3_1[0][0]  │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_2 (Conv1D) │ (None, 11, 256)   │    262,400 │ maxpool1d_1_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_2 (Conv1D) │ (None, 11, 256)   │    262,400 │ maxpool1d_2_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_2 (Conv1D) │ (None, 11, 256)   │    262,400 │ maxpool1d_3_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1_1         │ (None, 11, 256)   │          0 │ conv1d_1_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2_1         │ (None, 11, 256)   │          0 │ conv1d_2_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3_1         │ (None, 11, 256)   │          0 │ conv1d_3_2[0][0]  │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_1_3 (Conv1D) │ (None, 10, 512)   │    262,656 │ dropout_1_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_2_3 (Conv1D) │ (None, 10, 512)   │    262,656 │ dropout_2_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_3_3 (Conv1D) │ (None, 10, 512)   │    262,656 │ dropout_3_1[0][0] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_1_1           │ (None, 512)       │          0 │ conv1d_1_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_2_1           │ (None, 512)       │          0 │ conv1d_2_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ gap1d_3_1           │ (None, 512)       │          0 │ conv1d_3_3[0][0]  │
│ (GlobalMaxPooling1… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 1536)      │          0 │ gap1d_1_1[0][0],  │
│ (Concatenate)       │                   │            │ gap1d_2_1[0][0],  │
│                     │                   │            │ gap1d_3_1[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 64)        │     98,368 │ concatenate_1[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ output (Dense)      │ (None, 6)         │        390 │ dense_1[0][0]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 5,115,092 (19.51 MB)
 Trainable params: 1,705,030 (6.50 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 3,410,062 (13.01 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7f895bc1d180>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.10769633948802948
Test val_loss: 0.51334148645401
Train accuracy: 0.9661319255828857
Accuracy Score: 0.9518716577540107
F1 Score: 0.9377057125976194
Classification Report:
               precision    recall  f1-score   support

         0.0       1.00      0.97      0.99        40
         1.0       0.98      0.94      0.96        54
         2.0       0.93      1.00      0.97        42
         3.0       0.88      1.00      0.93        14
         4.0       1.00      0.80      0.89        15
         5.0       0.87      0.91      0.89        22

    accuracy                           0.95       187
   macro avg       0.94      0.94      0.94       187
weighted avg       0.95      0.95      0.95       187

Training History:
accuracy: [0.4313725531101227, 0.5668449401855469, 0.675579309463501, 0.7308377623558044, 0.7967914342880249, 0.8360071182250977, 0.8235294222831726, 0.8770053386688232, 0.9108734130859375, 0.9019607901573181, 0.8770053386688232, 0.8805704116821289, 0.9180035591125488, 0.9447415471076965, 0.926916241645813, 0.9358288645744324, 0.9358288645744324, 0.9376114010810852, 0.9376114010810852, 0.9429590106010437, 0.9590017795562744, 0.9625668525695801, 0.9411764740943909, 0.9643493890762329, 0.9625668525695801, 0.9661319255828857, 0.9572192430496216, 0.9714794754981995, 0.9714794754981995, 0.9572192430496216, 0.9518716335296631, 0.9607843160629272, 0.9661319255828857]
loss: [1.4188905954360962, 1.0372282266616821, 0.7831709980964661, 0.6859359741210938, 0.48844802379608154, 0.37655016779899597, 0.4510188102722168, 0.34559521079063416, 0.25361141562461853, 0.2637079358100891, 0.3202042281627655, 0.3432543873786926, 0.22207853198051453, 0.16924089193344116, 0.1803533285856247, 0.1610804796218872, 0.17803996801376343, 0.18233422935009003, 0.160624697804451, 0.14412295818328857, 0.12234446406364441, 0.11082234978675842, 0.1418539136648178, 0.11049385368824005, 0.08715654164552689, 0.08859851956367493, 0.11936444789171219, 0.08720967173576355, 0.07552740722894669, 0.14130155742168427, 0.15619635581970215, 0.0698046162724495, 0.10769633948802948]
val_accuracy: [0.49732619524002075, 0.47593581676483154, 0.7112299203872681, 0.6684492230415344, 0.7967914342880249, 0.7754010558128357, 0.7486631274223328, 0.8128342032432556, 0.8342245817184448, 0.8235294222831726, 0.7807486653327942, 0.7967914342880249, 0.8716577291488647, 0.8609625697135925, 0.8449198007583618, 0.8770053386688232, 0.8288770318031311, 0.8502673506736755, 0.8716577291488647, 0.8877005577087402, 0.893048107624054, 0.866310179233551, 0.8823529481887817, 0.8877005577087402, 0.8877005577087402, 0.866310179233551, 0.903743326663971, 0.8877005577087402, 0.8449198007583618, 0.8877005577087402, 0.8609625697135925, 0.903743326663971, 0.8716577291488647]
val_loss: [1.149284839630127, 0.9681646227836609, 0.9223358631134033, 0.8833330869674683, 0.5591846704483032, 0.6653992533683777, 0.6247459053993225, 0.608721911907196, 0.4716782569885254, 0.5661587715148926, 0.6128416657447815, 0.576459527015686, 0.42901116609573364, 0.4290332496166229, 0.46757781505584717, 0.49068307876586914, 0.6134994029998779, 0.6368056535720825, 0.4665858745574951, 0.40951424837112427, 0.36672553420066833, 0.49618175625801086, 0.4118253290653229, 0.44650137424468994, 0.5035069584846497, 0.6042167544364929, 0.40046414732933044, 0.5738507509231567, 0.6644122004508972, 0.5215966701507568, 0.5555350184440613, 0.5056297779083252, 0.51334148645401]

################################################################################################ 

