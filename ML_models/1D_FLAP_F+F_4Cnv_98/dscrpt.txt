def build_easyModel():
    input_x = Input(shape=(WS_B, 1))
    input_z = Input(shape=(WS_B, 1))

    # Branch for Force_X
    x = Conv1D(filters=256, kernel_size=20, strides=4, activation='relu')(input_x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(filters=256, kernel_size=16, strides=4, activation='relu')(x)
    x = Dropout(rate=0.1)(x)
    x = Conv1D(filters=128, kernel_size=8, activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(filters=64, kernel_size=4, activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Flatten()(x)

    # Branch for Force_Z
    z = Conv1D(filters=256, kernel_size=20, strides=4, activation='relu')(input_z)
    z = MaxPooling1D(pool_size=2)(z)
    z = Conv1D(filters=256, kernel_size=16, strides=4, activation='relu')(z)
    z = Dropout(rate=0.1)(z)
    z = Conv1D(filters=128, kernel_size=8, activation='relu')(z)
    z = MaxPooling1D(pool_size=2)(z)
    z = Conv1D(filters=64, kernel_size=4, activation='relu')(z)
    z = MaxPooling1D(pool_size=2)(z)
    z = Flatten()(z)

    # Merging branches
    merged = concatenate([x, z])
    common_layer = Dense(64, activation='relu')(merged)
    output_layer = Dense(N_CLASS, activation='softmax')(common_layer)

    model = Model(inputs=[input_x, input_z], outputs=output_layer)
    return model

Model: "functional_17"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer_14      │ (None, 1800, 1)   │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input_layer_15      │ (None, 1800, 1)   │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_52 (Conv1D)  │ (None, 446, 256)  │      5,376 │ input_layer_14[0… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_56 (Conv1D)  │ (None, 446, 256)  │      5,376 │ input_layer_15[0… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_42    │ (None, 223, 256)  │          0 │ conv1d_52[0][0]   │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_45    │ (None, 223, 256)  │          0 │ conv1d_56[0][0]   │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_53 (Conv1D)  │ (None, 52, 256)   │  1,048,832 │ max_pooling1d_42… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_57 (Conv1D)  │ (None, 52, 256)   │  1,048,832 │ max_pooling1d_45… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_12          │ (None, 52, 256)   │          0 │ conv1d_53[0][0]   │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_13          │ (None, 52, 256)   │          0 │ conv1d_57[0][0]   │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_54 (Conv1D)  │ (None, 45, 128)   │    262,272 │ dropout_12[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_58 (Conv1D)  │ (None, 45, 128)   │    262,272 │ dropout_13[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_43    │ (None, 22, 128)   │          0 │ conv1d_54[0][0]   │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_46    │ (None, 22, 128)   │          0 │ conv1d_58[0][0]   │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_55 (Conv1D)  │ (None, 19, 64)    │     32,832 │ max_pooling1d_43… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv1d_59 (Conv1D)  │ (None, 19, 64)    │     32,832 │ max_pooling1d_46… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_44    │ (None, 9, 64)     │          0 │ conv1d_55[0][0]   │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling1d_47    │ (None, 9, 64)     │          0 │ conv1d_59[0][0]   │
│ (MaxPooling1D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_16          │ (None, 576)       │          0 │ max_pooling1d_44… │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_17          │ (None, 576)       │          0 │ max_pooling1d_47… │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_8       │ (None, 1152)      │          0 │ flatten_16[0][0], │
│ (Concatenate)       │                   │            │ flatten_17[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_16 (Dense)    │ (None, 64)        │     73,792 │ concatenate_8[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_17 (Dense)    │ (None, 3)         │        195 │ dense_16[0][0]    │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 8,317,835 (31.73 MB)
 Trainable params: 2,772,611 (10.58 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 5,545,224 (21.15 MB)

Model Configuration:
Optimizer: <keras.src.optimizers.adam.Adam object at 0x7fa474429a80>
Loss Function: sparse_categorical_crossentropy
Learning Rate: <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>

Train loss: 0.11760853976011276
Test val_loss: 0.5216156840324402
Train accuracy: 0.9503546357154846
Accuracy Score: 0.9789473684210527
F1 Score: 0.9801169590643276
Classification Report:
               precision    recall  f1-score   support

         0.0       0.94      1.00      0.97        29
         1.0       1.00      1.00      1.00        27
         2.0       1.00      0.95      0.97        39

    accuracy                           0.98        95
   macro avg       0.98      0.98      0.98        95
weighted avg       0.98      0.98      0.98        95

Training History:
accuracy: [0.5496453642845154, 0.6843971610069275, 0.804964542388916, 0.8156028389930725, 0.8546099066734314, 0.9042553305625916, 0.9007092118263245, 0.9290780425071716, 0.9184397459030151, 0.8865247964859009, 0.9219858050346375, 0.9397163391113281, 0.9255319237709045, 0.9432623982429504, 0.9609929323196411, 0.9680851101875305, 0.9468085169792175, 0.9397163391113281, 0.9184397459030151, 0.9432623982429504, 0.9609929323196411, 0.9503546357154846]
loss: [1.0087900161743164, 0.7310886979103088, 0.5331644415855408, 0.4491814970970154, 0.3805725574493408, 0.340169757604599, 0.25563010573387146, 0.2028486579656601, 0.22984132170677185, 0.2559633255004883, 0.1892942190170288, 0.14868339896202087, 0.16947360336780548, 0.14717957377433777, 0.11082343757152557, 0.08461824059486389, 0.11725348979234695, 0.19609229266643524, 0.179184690117836, 0.14388002455234528, 0.11186110973358154, 0.11760853976011276]
val_accuracy: [0.7340425252914429, 0.7340425252914429, 0.7446808218955994, 0.8404255509376526, 0.8723404407501221, 0.8297872543334961, 0.8723404407501221, 0.8723404407501221, 0.914893627166748, 0.9042553305625916, 0.9255319237709045, 0.8936170339584351, 0.8404255509376526, 0.914893627166748, 0.9255319237709045, 0.8510638475418091, 0.8297872543334961, 0.9255319237709045, 0.8617021441459656, 0.8297872543334961, 0.9042553305625916, 0.8510638475418091]
val_loss: [0.6878365874290466, 0.6315436363220215, 0.5347492694854736, 0.4541149437427521, 0.40118396282196045, 0.4060218930244446, 0.4054901897907257, 0.33974871039390564, 0.40899938344955444, 0.33939260244369507, 0.357943058013916, 0.40795743465423584, 0.5326939225196838, 0.32252511382102966, 0.3896920680999756, 0.5127527713775635, 0.7140319347381592, 0.44165104627609253, 0.45362335443496704, 0.48473796248435974, 0.4004417061805725, 0.5216156840324402]

################################################################################################ 

